Problem Statement:
Linear regression by using Deep Neural network: Implement Boston housing price
prediction problem by Linear regression using Deep Neural network. Use Boston House
price prediction dataset.
Objective:
1. Pre-process the dataset
2. Identify outliers
3. Check the correlation
4. Implement linear regression using Deep Neural network
5. Predict the price of the house given the other features.
6. Evaluate the models and compare their respective scores like R2, RMSE

Theory:
Deep Learning for Linear regression:
Linear Regression
Linear Regression is a supervised learning technique that involves learning the relationship
between the features and the target. The target values are continuous, which means that the
values can take any values between an interval. For example, 1.2, 2.4, and 5.6 are continuous
values. Use-cases of regression include stock market price prediction, house price prediction,
sales prediction, etc.

The y hat is called the hypothesis function. The objective of linear regression is to learn the
parameters in the hypothesis function. The model parameters are intercept (beta 0) and the

ASSIGNMENT NO. 5

TITLE: LINEAR REGRESSION BY USING DEEP NEURAL NETWORK.

Laboratory Practice III(ML) Manual Dept of Computer Engineering

International Institute of Information Technology, Hinjawadi 2
slope (beta 1). The above equation is valid for univariate data, which means there is only one
column in the data as a feature.
How does linear regression learn the parameters?

The numerator denotes the covariance of the data and the denominator denotes the variance
of the feature X. The result will be the value of beta 1 which is also called the slope. The beta 1
parameter determines the slope of the linear regression line. The intercept decides where the
line should pass through in the y-axis.

In the image above the intercept-value would be 5, because it is the point where the linear
regression line passes through the y-axis. In this way, the linear regression learns the
relationship between the features and target.
Regression using Artificial Neural Networks
Why do we need to use Artificial Neural Networks for Regression instead of simply using Linear
Regression?
The purpose of using Artificial Neural Networks for Regression over Linear Regression is that
the linear regression can only learn the linear relationship between the features and target and
therefore cannot learn the complex non-linear relationship. To learn the complex non-linear
relationship between the features and target, we need other techniques. One of those
techniques is to use Artificial Neural Networks. Artificial Neural Networks can learn the complex

Laboratory Practice III(ML) Manual Dept of Computer Engineering

International Institute of Information Technology, Hinjawadi 3
relationship between the features and target due to the presence of activation function in each
layer. Let’s look at what are Artificial Neural Networks and how do they work.
Artificial Neural Networks
Artificial Neural Networks are one of the deep learning algorithms that simulate the workings of
neurons in the human brain. There are many types of Artificial Neural Networks, Vanilla Neural
Networks, Recurrent Neural Networks, and Convolutional Neural Networks. The Vanilla Neural
Networks can handle structured data only, whereas the Recurrent Neural Networks and
Convolutional Neural Networks can handle unstructured data very well. Vanilla Neural Networks
to perform the Regression Analysis.

The Artificial Neural Networks consists of the Input layer, Hidden layers, Output layer. The
hidden layer can be more than one in number. Each layer consists of n number of neurons.
Each layer will be having an Activation Function associated with each of the neurons. The
activation function is the function that is responsible for introducing non-linearity in the
relationship. In our case, the output layer must contain a linear activation function. Each layer
can also have regularizers associated with it. Regularizers are responsible for preventing
overfitting.
Artificial Neural Networks consists of two phases,
 Forward Propagation
 Backward Propagation
Forward propagation is the process of multiplying weights with each feature and adding them.
The bias is also added to the result. Backward propagation is the process of updating the

Laboratory Practice III(ML) Manual Dept of Computer Engineering

International Institute of Information Technology, Hinjawadi 4
weights in the model. Backward propagation requires an optimization function and a loss
function.
Steps to create an ANN model

Step 1 - Import the library
import pandas as pd
import numpy as np
from keras.datasets import mnist
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras. layers import Dense
from keras. layers import Dropout
Step 2 - Loading the Dataset
Here we have used the inbuilt mnist dataset and stored the train data in X_train and y_train.
We have used X_test and y_test to store the test data.
(X_train, y_train), (X_test, y_test) = mnist. load_data ()
Step 3 - Creating model and adding layers
We have created an object model for sequential model. We can use two args i.e layers and
name.
model = Sequential ()
Now, We are adding the layers by using &#39;add&#39;. We can specify the type of layer, activation
function to be used and many other things while adding the layer.
Here we have added four layers which will be connected one after other.
model.add (Dense(512))
model.add (Dropout (0.3))

model.add (Dense (256, activation=&#39;relu&#39;))

model.add (Dropout (0.2))
Step 4 - Compiling the model
We can compile a model by using compile attribute. Let us first look at its parameters before using it.
 optimizer: In this we can pass the optimizer we want to use. There are various optimizer like
SGD, Adam etc.
 loss: In this we can pass a loss function which we want for the model
 metrics : In this we can pass the metric on which we want the model to be scored
model. compile (optimizer=&#39;Adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
Step 5 - Fitting the model

Laboratory Practice III(ML) Manual Dept of Computer Engineering

International Institute of Information Technology, Hinjawadi 5
We can fit a model on the data we have and can use the model after that. Here we are using the
data which we have split i.e the training data for fitting the model.
While fitting we can pass various parameters like batch_size, epochs, verbose, validation_data
and so on.
model.fit (X_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(X_test,
y_test)
Step 6 - Evaluating the model
After fitting a model, we want to evaluate the model. Here we are using model.evaluate to evaluate
the model and it will give us the loss and the accuracy. Here we have also printed the score.
score = model. evaluate (X_test, y_test, verbose=0)
print (&#39;Test loss:&#39;, score [0])
print (&#39;Test accuracy:&#39;, score [1])

 
R² score tells us how well our model is fitted to the data by comparing it to the average line of
the dependent variable. If the score is closer to 1, then it indicates that our model performs
well versus if the score is farther from 1, then it indicates that our model does not perform so
well.
Conclusion: We studied how to use the Artificial Neural Network for implementing linear
regression model and Predict the price using Boston housing dataset . Also Evaluated the
models and compare their respective scores like R2, MAE

# import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# read the csv file 
Data = pd.read_csv('kc_house_data.csv')

Data.head(5).T

#get some information about our DataSet
Data.info()
Data.describe().transpose()

# let's drop unnecessory featurs
Data = Data.drop('id',axis=1)
Data = Data.drop('zipcode',axis=1)

# check if there are any Null values
Data.isnull().sum()

#visualizing house prices
fig = plt.figure(figsize=(10,7))
fig.add_subplot(2,1,1)
sns.distplot(Data['price'])
fig.add_subplot(2,1,2)
sns.boxplot(Data['price'])
plt.tight_layout()

#visualizing house prices
fig = plt.figure(figsize=(10,7))
fig.add_subplot(2,1,1)
sns.distplot(Data['price'])
fig.add_subplot(2,1,2)
sns.boxplot(Data['price'])
plt.tight_layout()

#visualizing square footage of (home,lot,above and basement)
fig = plt.figure(figsize=(16,5))
fig.add_subplot(2,2,1)
sns.scatterplot(Data['sqft_above'], Data['price'])
fig.add_subplot(2,2,2)
sns.scatterplot(Data['sqft_lot'],Data['price'])
fig.add_subplot(2,2,3)
sns.scatterplot(Data['sqft_living'],Data['price'])
fig.add_subplot(2,2,4)
sns.scatterplot(Data['sqft_basement'],Data['price'])

# check correlation
Data.corr()['price'].sort_values(ascending=False)

# feature with higher correlation
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='sqft_living',data=Data)

# feature like number of bedroom or bathroom
plt.figure(figsize = (10,8))
sns.boxplot(x = 'bedrooms',y = 'price', data = Data)

plt.figure(figsize=(15,10))
sns.scatterplot(x='long',y='lat',data=Data,hue='price')

#let's clean it the map a bit - we are taking 99% bottom price houses
non_top_1_perc = Data.sort_values('price',ascending = False).iloc[216:]

len(Data)*0.01

# let's try this one again
plt.figure(figsize=(15,10))
sns.scatterplot(x='long',y='lat',data=non_top_1_perc,alpha = 0.8,palette = 'RdYlGn', hue='price')

#let's break date to years, months
Data['date'] = pd.to_datetime(Data['date'])
Data['month'] = Data['date'].apply(lambda date:date.month)
Data['year'] = Data['date'].apply(lambda date:date.year)
Data.head(5)

# data visualization house price vs months and years
fig = plt.figure(figsize=(16,5))
fig.add_subplot(1,2,1)
Data.groupby('month').mean()['price'].plot()
fig.add_subplot(1,2,2)
Data.groupby('year').mean()['price'].plot()

Data = Data.drop('date',axis=1)

Scaling and Train Test Split
X = Data.drop('price',axis =1).values
y = Data['price'].values

#splitting Train and Test 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

Feature Scalling
#standardization scaler - fit&transform on train, fit only on test
from sklearn.preprocessing import StandardScaler
s_scaler = StandardScaler()
X_train = s_scaler.fit_transform(X_train.astype(np.float))
X_test = s_scaler.transform(X_test.astype(np.float))

Method1: Multiple Linear Regression
#Liner Regression
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()  
regressor.fit(X_train, y_train)

#evaluate the model (intercept and slope)
regressor.intercept_
regressor.coef_

y_predd = regressor.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_predd})
df1 = df.head(10)
df1

fig = plt.figure(figsize=(10,5))
residuals = (y_test- y_pred)
sns.distplot(residuals)

from sklearn import metrics

print('Mean Absolute Error: {:.2f}'.format(metrics.mean_absolute_error(y_test, y_predd))) 
print('Mean Squared Error:{:.2f}'.format(metrics.mean_squared_error(y_test, y_predd)))  
print('Root Mean Squared Error:{:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_predd))))
print('Variance score is: {:.2f}'.format(metrics.explained_variance_score(y_test,y_predd)))

# we are off about 20% (comparing mean absolut error and mean of price)
Data['price'].mean()

print('Linear Regression Model:')
print("Train Score {:.2f}".format(regressor.score(X_train,y_train)))
print("Test Score {:.2f}".format(regressor.score(X_test, y_test)))

Method2: Keras Regression
# Creating a Neural Network Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam

# having 19 nueron is based on the number of available featurs

model = Sequential()

model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')

Training the Model
model.fit(x=X_train,y=y_train,
          validation_data=(X_test,y_test),
          batch_size=128,epochs=400)

loss_df = pd.DataFrame(model.history.history)
loss_df.plot(figsize=(12,8))

Evaluation on Test Data
y_pred = model.predict(X_test)

# evaluation metrics
# explained variance score: best possible score is 1 and lower values are worse
from sklearn import metrics

print('Mean Absolute Error: {:.2f}'.format(metrics.mean_absolute_error(y_test, y_pred)))
print('Mean Squared Error: {:.2f}'.format(metrics.mean_squared_error(y_test, y_pred)))
print('Root Mean Squared Error: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
print('Variance score is: {:.2f}'.format(metrics.explained_variance_score(y_test,y_pred)))

# we are off about 20% (comparing mean absolut error and mean of price)
Data['price'].mean()

# Our predictions
fig = plt.figure(figsize=(10,5))
plt.scatter(y_test,y_pred)
# Perfect predictions
plt.plot(y_test,y_test,'r')

fig = plt.figure(figsize=(10,5))
residuals = (y_test- y_pred)
sns.distplot(residuals)

How we can use the model!
# let's drop price and keep first line featurs, then put those featur inside the model and estimae the price!
single_house = Data.drop('price',axis = 1).iloc[0]

# need to apply featur scalling on those featurs
single_house = s_scaler.transform(single_house.values.reshape(-1,19))

# apply model on those featurs
model.predict(single_house)

# compare the prediction with real price for the first column
Data['price'][0]

Multiple Linear Regression vs Keras Regression
print('Model: Keras Regression\n')

print('Mean Absolute Error(MAE): {:.2f}'.format(metrics.mean_absolute_error(y_test, y_pred)))
print('Mean Squared Error(MSE): {:.2f}'.format(metrics.mean_squared_error(y_test, y_pred)))
print('Root Mean Squared Error(RMSE): {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
print('Variance score: {:.2f}\n'.format(metrics.explained_variance_score(y_test,y_pred)*100))
print('*********************************\n')
print('Model: Multiple Linear Regression\n')
print('Mean Absolute Error(MAE): {:.2f}'.format(metrics.mean_absolute_error(y_test, y_predd))) 
print('Mean Squared Error(MSE):{:.2f}'.format(metrics.mean_squared_error(y_test, y_predd)))  
print('Root Mean Squared Error(RMSE):{:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_predd))))
print('Variance score: {:.2f}'.format(metrics.explained_variance_score(y_test,y_predd)*100))
