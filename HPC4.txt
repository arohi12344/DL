Aim: To write a CUDA program that, given an N-element vector, find.
Minimum element in vector
Maximum element in vector
Arithmetic mean of the vector
Standard deviation of the values in the vector
Objective: To study and implement the operations on vector, generate o/p as two computed
max values as well as time taken to find each value.

Outcome: Students will be understand the implementation of operations on vector, generate
o/p as two computed max with respect to time.
Pre-requisites:
64-bit Open source Linux or its derivative
Programming Languages: C/C++,CUDA
Theory:
Sequential Programming:

When solving a problem with a computer program, it is natural to divide the problem into a
discrete series of calculations; each calculation performs a specified task, as shown in
following Figure .Such a pro-gram is called a sequential program.

Laboratory Practice – I BE (Comp Engg)

[Type here]
.
Parallel Programming:
There are two fundamental types of parallelism in applications:
➤ Task parallelism
➤ Data parallelism
Task parallelism arises when there are many tasks or functions that can be operated
independently and largely in parallel. Task parallelism focuses on distributing
functions across multiple cores.

Data parallelism arises when there are many data items that can be operated on at the
same time.
Data parallelism focuses on distributing the data across multiple cores.

Laboratory Practice – I BE (Comp Engg)

[Type here]
CUDA :

CUDA programming is especially well-suited to address problems that can be
expressed as data-
parallel computations. Any applications that process large data sets can use a data-parallel
model
to speed up the computations. Data-parallel processing maps data elements to parallel
threads.
The first step in designing a data parallel program is to partition data across threads, with
each
thread working on a portion of the data.
The first step in designing a data parallel program is to partition data across threads, with
each
thread working on a portion of the data.

CUDA Architecture:

A heterogeneous application consists of two parts:
➤ Host code
➤ Device code
Host code runs on CPUs and device code runs on GPUs. An application executing on a
heterogeneous platform is typically initialized by the CPU. The CPU code is responsible for
managing the environment, code, and data for the device before loading compute-intensive
tasks on the device. With computational intensive applications, program sections often
exhibit a rich amount of data parallelism. GPUs are used to accelerate the execution of this
portion of data parallelism. When a hardware component that is physically separate from the
CPU is used to accelerate computationally intensive sections of an application, it is referred
to as a hardware accelerator. GPUs are arguably the most common example of a hardware
accelerator. GPUs must operate in conjunction with a CPU-based host through a PCI-Express
bus, as shown in Figure.

Laboratory Practice – I BE (Comp Engg)

[Type here]
NVIDIA’s CUDA nvcc compiler separates the device code from the host code during
the compilation process. The device code is written using CUDA C extended with
keywords for labeling data-parallel functions, called kernels . The device code is
further compiled by
Nvcc . During the link stage, CUDA runtime libraries are added for kernel procedure
calls and explicit GPU device manipulation. Further kernel function, named
helloFromGPU, to print the string of “Hello World from GPU!” as follows:
__global__ void helloFromGPU(void)
{
printf(“Hello World from GPU!\n”);
}
The qualifier __global__tells the compiler that the function will be called from the
CPU and exe-
cuted on the GPU. Launch the kernel function with the following code:

helloFromGPU &lt;&lt;&lt;1,10&gt;&gt;&gt;();

Triple angle brackets mark a call from the host thread to the code on the device side.
A kernel is executed by an array of threads and all threads run the same code. The
parameters within the triple angle brackets are the execution configuration, which
specifies how many threads will execute the kernel. In this example, you will run 10
GPU threads.

Laboratory Practice – I BE (Comp Engg)

[Type here]
A typical processing flow of a CUDA program follows this pattern:

1. Copy data from CPU memory to GPU memory.
2. Invoke kernels to operate on the data stored in GPU memory.
3. Copy data back from GPU memory to CPU memory

Table lists the standard C functions and their corresponding CUDA C functions for memory
operations. Host and Device Memory Functions are follows.

Organizing Threads:

When a kernel function is launched from the host side, execution is moved to a device where
a large number of threads are generated and each thread executes the statements specified by
the
kernel function. The two-level thread hierarchy decomposed into blocks of threads and grids
of blocks, as shown in following figure:

Laboratory Practice – I BE (Comp Engg)

[Type here]
All threads spawned by a single kernel launch are collectively called a grid . All
threads in a grid
share the same global memory space. A grid is made up of many thread blocks. A
thread block is a group of threads that can cooperate with each other using:
➤ Block-local synchronization
➤ Block-local shared memory
Threads from different blocks cannot cooperate.
Threads rely on the following two unique coordinates to distinguish themselves from
each other:

➤ blockIdx (block index within a grid)
➤ threadIdx (thread index within a block)
These variables appear as built-in, pre-initialized variables that can be accessed
within kernel functions. When a kernel function is executed, the coordinate variables
blockIdx and

Laboratory Practice – I BE (Comp Engg)

[Type here]
threadIdx are assigned to each thread by the CUDA runtime. Based on the coordinates, you
can assign portions of data to different threads. It is a structure containing three unsigned
integers, and the 1st, 2nd, and 3rd components are accessible through the fields x, y, and z
respectively.
blockIdx.x
blockIdx.y
blockIdx.z
threadIdx.x
threadIdx.y
threadIdx.z
CUDA organizes grids and blocks in three dimensions. The dimensions of a grid and a block
are specified by the following two built-in variables:
➤blockDim (block dimension, measured in threads)
➤gridDim (grid dimension, measured in blocks)
These variables are of type dim3, that is used to specify dimensions. When defining a
variable of type dim3, any component left unspecified is initialized to 1.Each component in a
variable of type dim3 is accessible through its x,y,, and z fields, respectively, as shown in the
following example:
blockDim.x
blockDim.y
blockDim.

CUDA program for calculating Min for given N-element vector
#include &lt;cuda.h&gt;
#include &lt;stdio.h&gt;
#include &lt;time.h&gt;
#define SIZE 100
__global__ void min(int *a , int *c)// kernel function definition
{
int i = threadIdx.x; // initialize i to thread ID
*c = a[55];
if(a[i] &lt; *c)
{

Laboratory Practice – I BE (Comp Engg)

[Type here]
*c = a[i];
}
}
int main()
{
int i;
srand(time(NULL)); //makes use of the computer&#39;s internal clock to
control the choice of the seed
int a[SIZE];
int c;
int *dev_a, *dev_c; //GPU / device parameters
cudaMalloc((void **) &amp;dev_a, SIZE*sizeof(int)); //assign memory to
parameters on GPU from CUDA runtime API
cudaMalloc((void **) &amp;dev_c, SIZE*sizeof(int));

for( i = 0 ; i &lt; SIZE ; i++)
{
a[i] = rand(); // input the numbers
}
for( i = 0 ; i &lt; SIZE ; i++)
{
printf(&quot;%d&quot;, a[i]); // input the numbers
}
cudaMemcpy(dev_a , a, SIZE*sizeof(int),cudaMemcpyHostToDevice);
//copy the array from CPU to GPU
min&lt;&lt;&lt;1,SIZE&gt;&gt;&gt;(dev_a,dev_c);

// call kernel function &lt;&lt;&lt;number of blocks, number of threads
cudaMemcpy(&amp;c, dev_c, SIZE*sizeof(int),cudaMemcpyDeviceToHost);
// copy the result back from GPU to CPU
printf(&quot;\nmin = %d &quot;,c);
cudaFree(dev_a); // Free the allocated memory
cudaFree(dev_c);
printf(&quot;&quot;);
return 0;
}
CUDA program for calculating Max for given N-element vector
#include &lt;cuda.h&gt;
#include &lt;stdio.h&gt;
#include &lt;time.h&gt;
#define SIZE 1000
__global__ void max(int *a , int *c) // kernel function definition

Laboratory Practice – I BE (Comp Engg)

[Type here]
{
int i = threadIdx.x; // initialize i to thread ID
*c = a[0];
if(a[i] &gt; *c)
{
*c = a[i];
}
}
int main()
{
int i;
srand(time(NULL)); //makes use of the computer&#39;s internal clock to
control the choice of the seed
int a[SIZE];
int c;
int *dev_a, *dev_c; //GPU / device parameters
cudaMalloc((void **) &amp;dev_a, SIZE*sizeof(int)); //assign memory
to parameters on GPU
cudaMalloc((void **) &amp;dev_c, SIZE*sizeof(int));
for( i = 0 ; i &lt; SIZE ; i++)
{
a[i] = i; // rand()% 1000 + 1; // input the numbers
}
cudaMemcpy(dev_a , a, SIZE*sizeof(int),cudaMemcpyHostToDevice);
//copy the array from CPU to GPU
max&lt;&lt;&lt;1,SIZE&gt;&gt;&gt;(dev_a,dev_c); // call kernel function &lt;&lt;&lt;number of
blocks, number of threads
cudaMemcpy(&amp;c, dev_c, SIZE*sizeof(int),cudaMemcpyDeviceToHost);
// copy the result back from GPU to CPU
printf(&quot;\nmax = %d &quot;,c);
cudaFree(dev_a); // Free the allocated memory
cudaFree(dev_c);
printf(&quot;&quot;);
return 0;
}
CUDA program for calculating standard deviation for given N-element
vector
#include &lt;stdio.h&gt;

Laboratory Practice – I BE (Comp Engg)

[Type here]
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
// CUDA kernel. Each thread takes care of one element of c
__global__ void vecAdd(int *a, int *c, int n)
{
// Get our global thread ID
int id = blockIdx.x*blockDim.x+threadIdx.x;
// c[id]=0;
// Make sure we do not go out of bounds
if (id &lt; n)
*c+= a[id];
// printf(&quot;\n%d&quot;, c[id]);
}
int main( int argc, char* argv[] )
{
// Size of vectors
// int n = 100000;
int n=5;
const int size = n * sizeof(int);
// Host input vectors
int *h_a;
// double *h_b;
//Host output vector
int *h_c;
// Device input vectors
int *d_a;
//double *d_b;
//Device output vector
int *d_c;
int dev=0;
// Size, in bytes, of each vector
size_t bytes = n*sizeof(double);
// Allocate memory for each vector on host
//h_a = (int*)malloc(bytes);
//h_b = (double*)malloc(bytes);
h_c = (int*)malloc(bytes);
// Allocate memory for each vector on GPU
cudaMalloc(&amp;d_a, bytes);
// cudaMalloc(&amp;d_b, bytes);
cudaMalloc(&amp;d_c, bytes);
int i;
printf(&quot;Input array&quot;);
// Initialize vectors on host
/*for( i = 0; i &lt; n; i++ ) {
// h_a[i] = sin(i)*sin(i);
//printf(&quot;\n&quot;,i);
h_a[i]=i;
//printf(&quot;\n%d&quot;, h_a[i]);
//h_b[i]=i;
//h_b[i] = cos(i)*cos(i);
}*/

Laboratory Practice – I BE (Comp Engg)

[Type here]
int a[]= {0, 1, 2, 3, 4};
cudaMalloc(&amp;h_a, size);
// Copy host vectors to device
cudaMemcpy( h_a, a, bytes, cudaMemcpyHostToDevice);
cudaMemcpy( d_c, &amp;dev, sizeof(int), cudaMemcpyHostToDevice);
// cudaMemcpy( d_b, h_b, bytes, cudaMemcpyHostToDevice);
int blockSize, gridSize;
// Number of threads in each thread block
blockSize = 2;
// Number of thread blocks in grid
gridSize = (int)ceil((float)n/blockSize);
// Execute the kernel
vecAdd&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_a,d_c,n);
int result;
// Copy array back to host
cudaMemcpy( &amp;result,d_c, sizeof(int), cudaMemcpyDeviceToHost );
// Sum up vector c and print result divided by n, this should equal 1
within error
double sum = 0;
//for(i=0; i&lt;n; i++)
// sum += h_c[i];
printf(&quot;final result: %f\n&quot;,result );
// vecdev&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_a,d_c, n);
// Release device memory
cudaFree(d_a);
//cudaFree(d_b);
cudaFree(d_c);
// Release host memory
free(h_a);
//free(h_b);
free(h_c);
return 0;
}
CUDA program for calculating arithmetic mean for given N-element
vector
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
// CUDA kernel. Each thread takes care of one element of c
__global__ void vecAdd(double *a, double *b, double *c, int n)
{

Laboratory Practice – I BE (Comp Engg)

[Type here]
// Get our global thread ID
int id = blockIdx.x*blockDim.x+threadIdx.x; // get global
index
// Make sure we do not go out of bounds
if (id &lt; n)
c[id] = a[id] + b[id];
}
int main( )
{
//int n = 100000;
int n=5; // Size of vectors
double *h_a; // Host input vector
double *h_b; // Host input vector
double *h_c; //Host output vector
double *d_a; // Device input vector
double *d_b; // Device input vector
double *d_c; //Device output vector
size_t bytes = n*sizeof(double); // Size, in bytes, of
each vector
// Allocate memory for each vector on host
h_a = (double*)malloc(bytes);
h_b = (double*)malloc(bytes);
h_c = (double*)malloc(bytes);
// Allocate memory for each vector on GPU
cudaMalloc(&amp;d_a, bytes);
cudaMalloc(&amp;d_b, bytes);
cudaMalloc(&amp;d_c, bytes);
int i;
// Initialize vectors on host
for( i = 0; i &lt; n; i++ ) {
h_a[i]=i;
h_b[i]=i;
}
// Copy host vectors to device
cudaMemcpy( d_a, h_a, bytes, cudaMemcpyHostToDevice);
cudaMemcpy( d_b, h_b, bytes, cudaMemcpyHostToDevice);
int blockSize, gridSize;

Laboratory Practice – I BE (Comp Engg)

[Type here]
// Number of threads in each thread block
blockSize = 1024;
// Number of thread blocks in grid
gridSize = (int)ceil((float)n/blockSize);
// Execute the kernel
vecAdd&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_a, d_b, d_c, n);
// Copy array back to host
cudaMemcpy( h_c, d_c, bytes, cudaMemcpyDeviceToHost );
// Sum up vector c and print result divided by n, this should equal 1
within error
double sum = 0;
for(i=0; i&lt;n; i++)
sum += h_c[i];
printf(&quot;Average mean of 2 vectors: %f\n&quot;, sum/n);
// Release device memory
cudaFree(d_a);
cudaFree(d_b);
cudaFree(d_c);
// Release host memory
free(h_a);
free(h_b);
free(h_c);
return 0;
}
Conclusion: We have implemented CUDA program for calculating Min, Max,
Arithmetic mean and Standard deviation Operations on N-element vector.
